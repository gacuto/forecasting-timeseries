{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Timeseries Forecasting with Nixtla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing Requirements\n",
    "\n",
    "Activate a Python environment and install the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install pandas\n",
    "%pip install pandas\n",
    "%pip install numpy\n",
    "%pip install darts\n",
    "%pip install matplotlib\n",
    "%pip install gluonts[torch] optuna\n",
    "%pip install ipykernel\n",
    "%pip install --upgrade nbformat\n",
    "%pip install lightgbm xgboost\n",
    "%pip install seaborn\n",
    "%pip install distributed\n",
    "%pip install datasetsforecast\n",
    "%pip install mlforecast\n",
    "%pip install statsforecast\n",
    "%pip install tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "In this tutorial, we will train and evaluate multiple time-series forecasting models using the [Store Item Demand Forecasting Challenge dataset from Kaggle](https://www.kaggle.com/competitions/demand-forecasting-kernels-only/data?select=train.csv). This dataset has 10 different stores and each store has 50 items, i.e. total of 500 daily level time series data for five years (2013â€“2017)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download data\n",
    "\n",
    "* Download the train.csv from https://www.kaggle.com/competitions/demand-forecasting-kernels-only/data?select=train.csv.\n",
    "* Create a `./data` directory inside the directory of this Python notebook\n",
    "* Save the train.csv inside the `./data` directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./data/train.csv\", index_col=0)\n",
    "data = data.reset_index()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The dataset has {data.shape[0]} rows and {data.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data fields\n",
    "\n",
    "* date - Date of the sale data. There are no holiday effects or store closures.\n",
    "* store - Store ID\n",
    "* item - Item ID\n",
    "* sales - Number of items sold at a particular store on a particular date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We don't have any negative \"sales\" values\n",
    "* We need to convert \"store\" and \"item\" columns to type string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce the amount of data\n",
    "\n",
    "For simplicity & speed, we will only keep data from the first 2 out of the 10 stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.loc[data[\"store\"] <= 2]\n",
    "print(f\"The dataset now has {data.shape[0]} rows and {data.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert \"store\" and \"item\" columns to type string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[[\"store\", \"item\"]] = data[[\"store\", \"item\"]].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for duplicate rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_duplicate_rows = len(data)-len(data.drop_duplicates())\n",
    "print(f\"There are {count_duplicate_rows} duplicate rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a new column by concatenating \"store\" and \"item\" columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"store_item\"] = data[\"store\"] + \"_\" + data[\"item\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert date column to datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"date\"] = pd.to_datetime(data[\"date\"]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate total sales per date, store and item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_sales_per_date = data.groupby([\"date\",\"store_item\"])[\"sales\"].aggregate(\"sum\")\n",
    "item_sales_per_date = item_sales_per_date.reset_index()\n",
    "item_sales_per_date.columns = [\"date\",\"store_item\",\"sales\"]\n",
    "item_sales_per_date = item_sales_per_date.sort_values(\"date\", ascending=True)\n",
    "item_sales_per_date.head()\n",
    "item_sales_per_date.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot total sales for all products over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_sales_per_date = item_sales_per_date.groupby([\"date\"])[\"sales\"].aggregate(\"sum\")\n",
    "total_sales_per_date.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Not all items sell every day\n",
    "\n",
    "* For each \"store_item\" value add all the missing \"date\" values and fill in \"sales\" with 0s\n",
    "## Generate an index with all combinations of \"date\" and \"store_item\" values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiindex = list(zip(item_sales_per_date[\"date\"], item_sales_per_date[\"store_item\"]))\n",
    "multiindex = pd.MultiIndex.from_tuples(multiindex, names=('index_1', 'index_2'))\n",
    "dataset = item_sales_per_date.copy()\n",
    "dataset.index = multiindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "idx_dates = list(pd.date_range(min(item_sales_per_date[\"date\"]), max(item_sales_per_date[\"date\"])))\n",
    "idx_ids = list(dataset[\"store_item\"].unique())\n",
    "idx = list(itertools.product(idx_dates, idx_ids))\n",
    "dataset = dataset.reindex(index=idx, columns=None)\n",
    "dataset = dataset.reset_index()\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For each \"store_item\" value fill add all the missing \"date\" values and fill in \"sales\" with 0s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values in 'date' column with values from 'index_1' column\n",
    "dataset[\"date\"].fillna(dataset[\"index_1\"], inplace=True)\n",
    "\n",
    "# Fill missing values in 'store_item' column with values from 'index_2' column\n",
    "dataset[\"store_item\"].fillna(dataset[\"index_2\"], inplace=True)\n",
    "\n",
    "# Fill missing values in 'sales' column with 0\n",
    "dataset[\"sales\"].fillna(0, inplace=True)\n",
    "\n",
    "# Drop 'index_1' and 'index_2' columns\n",
    "dataset.drop(columns=[\"index_1\", \"index_2\"], inplace=True)\n",
    "\n",
    "# Set 'date' column as the index and rename index to \"date\"\n",
    "dataset.set_index(\"date\", inplace=True)\n",
    "dataset.index.name = \"date\"\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the dataset that we will use for training the forecasting models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset that we will use for training the forecasting models\n",
    "# Rename columns to match the Nixtlaverse's expectations\n",
    "# The 'store_item' becomes 'unique_id' representing the unique identifier of the time series\n",
    "# The 'date' becomes 'ds' representing the time stamp of the data points\n",
    "# The 'sales' becomes 'y' representing the target variable we want to forecast\n",
    "\n",
    "# Select the entire dataset for training\n",
    "Y_df = dataset.copy()\n",
    "\n",
    "# Reset the index to have a default integer index\n",
    "Y_df.reset_index(inplace=True)\n",
    "\n",
    "# Rename columns according to the requirements\n",
    "Y_df.rename(columns={'store_item': 'unique_id', 'date': 'ds', 'sales': 'y'}, inplace=True)\n",
    "\n",
    "# Convert the 'y' column to integer type\n",
    "Y_df['y'] = Y_df['y'].astype(int)\n",
    "\n",
    "# Convert the 'unique_id' column to string type\n",
    "Y_df['unique_id'] = Y_df['unique_id'].astype(str)\n",
    "\n",
    "# Convert the 'ds' column to datetime format to ensure proper handling of date-related operations in subsequent steps\n",
    "Y_df['ds'] = pd.to_datetime(Y_df['ds'])\n",
    "\n",
    "# Display the last few rows of the DataFrame to verify the changes\n",
    "Y_df.tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.autonotebook import tqdm\n",
    "from statsforecast import StatsForecast\n",
    "# Feature: plot random series for EDA\n",
    "StatsForecast.plot(Y_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check for seasonality in the total number of 'sales' per 'date'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import darts\n",
    "from darts import TimeSeries\n",
    "\n",
    "seasonality_check_data = total_sales_per_date.reset_index()\n",
    "seasonality_check_data = seasonality_check_data.set_index(\"date\")\n",
    "seasonality_check_data_series = TimeSeries.from_times_and_values(seasonality_check_data.index, seasonality_check_data[\"sales\"].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autocorrelation function \n",
    "Autocorrelation ([source](https://www.investopedia.com/terms/a/autocorrelation.asp)) is a mathematical representation of the degree of similarity between a given time series and a lagged version of itself over successive time intervals. It's conceptually similar to the correlation between two different time series, but autocorrelation uses the same time series twice: once in its original form and once lagged one or more time periods. \n",
    "\n",
    "* Autocorrelation represents the degree of similarity between a given time series and a lagged version of itself over successive time intervals.\n",
    "* Autocorrelation measures the relationship between a variable's current value and its past values.\n",
    "* An autocorrelation of +1 represents a perfect positive correlation, while an autocorrelation of -1 represents a perfect negative correlation.\n",
    "\n",
    "The plot_acf function is called with the following parameters:\n",
    "\n",
    "* **seasonality_check_data_series**: This is the time series data for which the autocorrelation function will be plotted.\n",
    "* **m=7**: This parameter specifies the period for seasonal decomposition. Here, m=7 indicates a weekly seasonality trend.\n",
    "* **alpha=0.05**: This parameter determines the significance level for the confidence interval. A value of 0.05 corresponds to a 95% confidence level.\n",
    "* **max_lag=30**: This parameter sets the maximum lag to consider in the autocorrelation function plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from darts.utils.statistics import plot_acf\n",
    "\n",
    "plot_acf(seasonality_check_data_series, m=7, alpha=0.05, max_lag=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ACF presents a spike at x in [1, 7, 14, 21], which suggests a weekly seasonality trend (highlighted). The blue zone determines the significance of the statistics for a confidence level of $\\alpha = 5\\%$. We can also run a statistical check of seasonality for each candidate period `m`.\n",
    "\n",
    "The confidence interval of the autocorrelation function provides a range of values within which the true autocorrelation of a time series is likely to fall.\n",
    "\n",
    "In simple terms, imagine you have a set of data points that represent measurements taken over time. The autocorrelation function tells you how related each data point is to the ones that came before it. The confidence interval gives you an idea of how reliable that measurement of autocorrelation is.\n",
    "\n",
    "The confidence interval is expressed as a range of values around the calculated autocorrelation. For example, if the autocorrelation of a specific lag (time gap between observations) is 0.7 and the confidence interval is Â±0.1, it means we are reasonably confident that the true autocorrelation lies between 0.6 and 0.8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" By setting max_lag=12*30, you're considering a maximum lag of 12 months, which allows you to analyze the autocorrelation over\n",
    "    a longer time period, potentially capturing seasonal patterns or long-term trends in the data. \n",
    "\"\"\"\n",
    "plot_acf(seasonality_check_data_series, m=7, alpha=0.05, max_lag=12*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now train multiple Statistical & ML models and evaluate which one performs best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create forecasts with Stats & ML methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stats Methods with StatsForecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary models from the statsforecast library\n",
    "from statsforecast.models import (\n",
    "    # ADIDA: Adaptive combination of Intermittent Demand Approaches, a model designed for intermittent demand\n",
    "    ADIDA,\n",
    "    # AutoETS: Automated Exponential Smoothing model that automatically selects the best Exponential Smoothing model based on AIC\n",
    "    AutoETS,\n",
    "    # CrostonOptimized: A model specifically designed for intermittent demand forecasting\n",
    "    CrostonOptimized,\n",
    "    # HistoricAverage: This model uses the average of all historical data as the forecast\n",
    "    HistoricAverage,\n",
    "    # IMAPA: Intermittent Multiplicative AutoRegressive Average, a model for intermittent series that incorporates autocorrelation\n",
    "    IMAPA,\n",
    "    # Naive: A simple model that uses the last observed value as the forecast\n",
    "    Naive,\n",
    "    # SeasonalNaive: A model that uses the previous season's data as the forecast\n",
    "    SeasonalNaive,\n",
    ")\n",
    "\n",
    "# Define the number of days in the future for which we will make a forecast\n",
    "horizon = 30\n",
    "\n",
    "# Define the length of the seasonality window (e.g., 7 days for weekly seasonality)\n",
    "season_length = 7\n",
    "\n",
    "# Define the number of days that the model will use to make a forecast (e.g., 6 months)\n",
    "window_size = 6 * 30\n",
    "\n",
    "# Define a list of forecasting models to be used\n",
    "models = [\n",
    "    # SeasonalNaive: A model that uses the previous season's data as the forecast\n",
    "    SeasonalNaive(season_length=season_length),\n",
    "    \n",
    "    # Naive: A simple model that uses the last observed value as the forecast\n",
    "    Naive(),\n",
    "    \n",
    "    # HistoricAverage: This model uses the average of all historical data as the forecast\n",
    "    HistoricAverage(),\n",
    "    \n",
    "    # CrostonOptimized: A model specifically designed for intermittent demand forecasting\n",
    "    CrostonOptimized(),\n",
    "    \n",
    "    # ADIDA: Adaptive combination of Intermittent Demand Approaches, a model designed for intermittent demand\n",
    "    ADIDA(),\n",
    "    \n",
    "    # IMAPA: Intermittent Multiplicative AutoRegressive Average, a model for intermittent series that incorporates autocorrelation\n",
    "    IMAPA(),\n",
    "    \n",
    "    # AutoETS: Automated Exponential Smoothing model that automatically selects the best Exponential Smoothing model based on AIC\n",
    "    AutoETS(season_length=season_length)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the StatsForecast class for forecasting\n",
    "sf = StatsForecast(\n",
    "    models=models,  # A list of forecasting models to be used\n",
    "    freq='D',       # The frequency of the time series data ('D' stands for daily frequency)\n",
    "    n_jobs=-1,      # The number of CPU cores to use for parallel execution (-1 means use all available cores)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from statsforecast.utils import ConformalIntervals\n",
    "\n",
    "# Create an instance of ConformalIntervals with the desired parameters\n",
    "prediction_intervals = ConformalIntervals()\n",
    "\n",
    "# Get the current time before forecasting starts, this will be used to measure the execution time\n",
    "init = time()\n",
    "\n",
    "# Call the forecast method of the StatsForecast instance to predict the next horizon days\n",
    "# Level is set to [90], which means that it will compute the 90% prediction interval\n",
    "# prediction_intervals is set to True to compute prediction intervals\n",
    "fcst_df = sf.forecast(df=Y_df, h=horizon, level=[90], prediction_intervals=prediction_intervals)\n",
    "\n",
    "# Get the current time after the forecasting ends\n",
    "end = time()\n",
    "\n",
    "# Calculate and print the total time taken for the forecasting in minutes\n",
    "print(f'Forecast Minutes: {(end - init) / 60}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcst_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Methods with MLForecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlforecast import MLForecast\n",
    "from mlforecast.target_transforms import Differences\n",
    "from mlforecast.utils import PredictionIntervals\n",
    "from window_ops.expanding import expanding_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary models from various libraries\n",
    "\n",
    "# LGBMRegressor: A gradient boosting framework that uses tree-based learning algorithms from the LightGBM library\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# XGBRegressor: A gradient boosting regressor model from the XGBoost library\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# LinearRegression: A simple linear regression model from the scikit-learn library\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlforecast import MLForecast\n",
    "from mlforecast.target_transforms import Differences\n",
    "from mlforecast.utils import PredictionIntervals\n",
    "from window_ops.expanding import expanding_mean\n",
    "# Instantiate the MLForecast object\n",
    "mlf = MLForecast(\n",
    "    models=[LGBMRegressor(max_depth=10), XGBRegressor(max_depth=10, eval_metric='rmse'), LinearRegression()],  # List of models for forecasting: LightGBM, XGBoost and Linear Regression\n",
    "    freq='D',  # Frequency of the data - 'D' for daily frequency\n",
    "    lags=list(range(1, 7)),  # Specific lags to use as regressors: 1 to 6 days\n",
    "    lag_transforms = {\n",
    "        1:  [expanding_mean],  # Apply expanding mean transformation to the lag of 1 day\n",
    "    },\n",
    "    date_features=['year', 'month', 'day', 'dayofweek', 'quarter', 'week', 'dayofyear'],  # Date features to use as regressors\n",
    "    target_transforms=[Differences([1])],\n",
    "\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the timer to calculate the time taken for fitting the models\n",
    "init = time()\n",
    "\n",
    "# Fit the MLForecast models to the data, with prediction intervals set using a window size of window_size days\n",
    "mlf.fit(Y_df,  prediction_intervals=PredictionIntervals(n_windows=10, h=48))\n",
    "\n",
    "# Calculate the end time after fitting the models\n",
    "end = time()\n",
    "\n",
    "# Print the time taken to fit the MLForecast models, in minutes\n",
    "print(f'MLForecast Minutes: {(end - init) / 60}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcst_mlf_df = mlf.predict(h=horizon, level=[90])\n",
    "fcst_mlf_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecast plots \n",
    "\n",
    "Per store, item and forecast model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcst_df = fcst_df.merge(fcst_mlf_df, how='left', on=['unique_id', 'ds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf.plot(Y_df, fcst_df, max_insample_length=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate Modelâ€™s Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation in StatsForecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = time()\n",
    "cv_df = sf.cross_validation(df=Y_df, h=horizon, n_windows=3, step_size=horizon, level=[90])\n",
    "end = time()\n",
    "print(f'CV Minutes: {(end - init) / 60}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = time()\n",
    "cv_mlf_df = mlf.cross_validation(\n",
    "    df=Y_df, \n",
    "    h=horizon, \n",
    "    n_windows=3, \n",
    "    step_size=horizon, \n",
    "    level=[90],\n",
    ")\n",
    "end = time()\n",
    "print(f'CV Minutes: {(end - init) / 60}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_df = cv_df.merge(cv_mlf_df.drop(columns=['y']), how='left', on=['unique_id', 'ds', 'cutoff'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoffs = cv_df['cutoff'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize cross validation splits for a specific unique_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cutoff in cutoffs:\n",
    "    img = sf.plot(\n",
    "        Y_df, \n",
    "        cv_df.query('cutoff == @cutoff').drop(columns=['y', 'cutoff']), \n",
    "        max_insample_length=30*4, \n",
    "        unique_ids=['1_1'],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregate sales forecasts for all items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_cv_df = cv_df.loc[:,~cv_df.columns.str.contains('hi|lo')].groupby(['ds', 'cutoff']).sum(numeric_only=True).reset_index()\n",
    "agg_cv_df.insert(0, 'unique_id', 'Total sales (all items)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_Y_df = Y_df.groupby(['ds']).sum(numeric_only=True).reset_index()\n",
    "agg_Y_df.insert(0, 'unique_id', 'Total sales (all items)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cutoff in cutoffs:\n",
    "    img = sf.plot(\n",
    "        agg_Y_df, \n",
    "        agg_cv_df.query('cutoff == @cutoff').drop(columns=['y', 'cutoff']),\n",
    "        max_insample_length=30*4,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evalute forecasts per store_item and CV window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Callable\n",
    "\n",
    "from distributed import Client\n",
    "from fugue import transform\n",
    "from fugue_dask import DaskExecutionEngine\n",
    "from datasetsforecast.losses import mse, mae, smape, mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(df: pd.DataFrame, metrics: List[Callable]) -> pd.DataFrame:\n",
    "    eval_ = {}\n",
    "    models = df.loc[:, ~df.columns.str.contains('unique_id|y|ds|cutoff|lo|hi')].columns\n",
    "    for model in models:\n",
    "        eval_[model] = {}\n",
    "        for metric in metrics:\n",
    "            eval_[model][metric.__name__] = metric(df['y'], df[model])\n",
    "    eval_df = pd.DataFrame(eval_).rename_axis('metric').reset_index()\n",
    "    eval_df.insert(0, 'cutoff', df['cutoff'].iloc[0])\n",
    "    eval_df.insert(0, 'unique_id', df['unique_id'].iloc[0])\n",
    "    return eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_models = cv_df.loc[:, ~cv_df.columns.str.contains('unique_id|y|ds|cutoff|lo|hi')].columns\n",
    "str_models = ','.join([f\"{model}:float\" for model in str_models])\n",
    "cv_df['cutoff'] = cv_df['cutoff'].astype(str)\n",
    "cv_df = cv_df.reset_index()\n",
    "cv_df['unique_id'] = cv_df['unique_id'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Letâ€™s create a dask client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client() # without this, dask is not in distributed mode\n",
    "# fugue.dask.dataframe.default.partitions determines the default partitions for a new DaskDataFrame\n",
    "engine = DaskExecutionEngine({\"fugue.dask.dataframe.default.partitions\": 96})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transform function takes the evaluate functions and applies it to each combination of time series (unique_id) and cross validation window (cutoff) using the dask client we created before.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_df = transform(\n",
    "    cv_df.loc[:, ~cv_df.columns.str.contains('lo|hi')], \n",
    "    evaluate, \n",
    "    engine=\"dask\",\n",
    "    params={'metrics': [mse, mae, mape, smape]}, \n",
    "    schema=f\"unique_id:str,cutoff:str,metric:str, {str_models}\", \n",
    "    as_local=True,\n",
    "    partition={'by': ['unique_id', 'cutoff']}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean metric for each cross validation window\n",
    "evaluation_df.groupby(['cutoff', 'metric']).mean(numeric_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distribution of errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_df_melted = pd.melt(evaluation_df, id_vars=['unique_id', 'cutoff', 'metric'], var_name='model', value_name='error')\n",
    "evaluation_df_melted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution of SMAPE errors per model, unique_id and evaluation metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.violinplot(evaluation_df_melted.query('metric==\"mse\"'), x='error', y='model').set_title('Distribution of MSE errors')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution of SMAPE errors per model, unique_id and evaluation metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.violinplot(evaluation_df_melted.query('metric==\"smape\"'), x='error', y='model').set_title('Distribution of SMAPE errors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_models = list(evaluation_df_melted.model.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose models for groups of series\n",
    "\n",
    "We can find the best performing model per store_item and evaluation metric.\n",
    "In how many cross validation fold & metric is each model overperforming the rest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the best model for each time series, metric, and cross validation window\n",
    "evaluation_df['best_model'] = evaluation_df.idxmin(axis=1, numeric_only=True)\n",
    "# count how many times a model wins per metric and cross validation window\n",
    "count_best_model = evaluation_df.groupby(['unique_id', 'metric', 'best_model']).size().rename('n').to_frame().reset_index()\n",
    "# plot results\n",
    "sns.barplot(count_best_model, x='n', y='best_model', hue='metric')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AutoETS is the best performing model for all evaluation metrics\n",
    "\n",
    "This does not mean that AutoETS is the best performing model for each individual \"store_item\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the best model for store_item=\"1_1\" sales forecasting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the mse, calculate how many times a model wins\n",
    "# add colors\n",
    "colors = ['#ff9999','#66b3ff','#99ff99','#ffcc99']\n",
    "count_best_model_mse = count_best_model.query('metric == \"mse\" & unique_id == \"1_1\"')\n",
    "counts_series_mse = count_best_model_mse[\"n\"]\n",
    "plt.pie(counts_series_mse, labels=count_best_model_mse[\"best_model\"], autopct='%.0f%%', colors=colors)\n",
    "plt.title(\"Winning Models Based On MSE - Store_item='1_1'\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the mse, calculate how many times a model wins\n",
    "count_best_model_mape = count_best_model.query('metric == \"mape\" & unique_id == \"1_1\"')\n",
    "counts_series_mape = count_best_model_mape[\"n\"]\n",
    "plt.pie(counts_series_mape, labels=count_best_model_mape[\"best_model\"], autopct='%.0f%%', colors=colors)\n",
    "plt.title(\"Winning Models Per CV Fold Based On MAPE - Store_item='1_1'\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBRegressor was the best performing model based on MSE for 2 out of the 3 validation folds of store_item 1_1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the mse, calculate how many times a model wins\n",
    "count_best_model_smape = count_best_model.query('metric == \"smape\" & unique_id == \"1_1\"')\n",
    "counts_series_smape = count_best_model_smape[\"n\"]\n",
    "plt.pie(counts_series_smape, labels=count_best_model_smape[\"best_model\"], autopct='%.0f%%', colors=colors)\n",
    "plt.title(\"Winning Models Per CV Fold Based On SMAPE - Store_item='1_1'\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LGBMRegressor was the best performing model based on MSE for 2 out of the 3 validation folds of store_item 1_1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the forecasts of the best model for unique_id == \"1_1\"\n",
    "\n",
    "* The best model based on MAPE is XGBRegressor\n",
    "* The best model based on SMAPE is LGBMRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_series_df = evaluation_df.query('metric == \"mape\" & unique_id == \"1_1\"').groupby(['unique_id']).mean(numeric_only=True)\n",
    "eval_series_df['best_model'] = eval_series_df.idxmin(axis=1)\n",
    "\n",
    "sf.plot(Y_df, cv_df.drop(columns=['cutoff', 'y']), \n",
    "        max_insample_length=30 * 2, \n",
    "        models=['XGBRegressor'],\n",
    "        unique_ids=eval_series_df.query('best_model == \"XGBRegressor\"').index[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_series_df = evaluation_df.query('metric == \"smape\" & unique_id == \"1_1\"').groupby(['unique_id']).mean(numeric_only=True)\n",
    "eval_series_df['best_model'] = eval_series_df.idxmin(axis=1)\n",
    "\n",
    "sf.plot(Y_df, cv_df.drop(columns=['cutoff', 'y']), \n",
    "        max_insample_length=30 * 2, \n",
    "        models=['LGBMRegressor'],\n",
    "        unique_ids=eval_series_df.query('best_model == \"LGBMRegressor\"').index[:8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the AutoETS forecasts for more unique_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_series_df = evaluation_df.query('metric == \"mape\"').groupby(['unique_id']).mean(numeric_only=True)\n",
    "eval_series_df['best_model'] = eval_series_df.idxmin(axis=1)\n",
    "\n",
    "sf.plot(Y_df, cv_df.drop(columns=['cutoff', 'y']), \n",
    "        max_insample_length=30 * 2, \n",
    "        models=['AutoETS'],\n",
    "        unique_ids=eval_series_df.query('best_model == \"AutoETS\"').index[:8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the XGBRegressor forecasts for more unique_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_series_df = evaluation_df.query('metric == \"mape\"').groupby(['unique_id']).mean(numeric_only=True)\n",
    "eval_series_df['best_model'] = eval_series_df.idxmin(axis=1)\n",
    "\n",
    "sf.plot(Y_df, cv_df.drop(columns=['cutoff', 'y']), \n",
    "        max_insample_length=30 * 2, \n",
    "        models=['XGBRegressor'],\n",
    "        unique_ids=eval_series_df.query('best_model == \"XGBRegressor\"').index[:8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sources\n",
    "\n",
    "This code is based on the following publicly available resources\n",
    "* [Nixtla Statistical, Machine Learning and Neural Forecasting methods](https://nixtla.github.io/statsforecast/docs/tutorials/statisticalneuralmethods.html)\n",
    "* [Intro to Forecasting with Darts](https://github.com/unit8co/darts/blob/master/examples/00-quickstart.ipynb)\n",
    "* [Store Item Demand Forecasting Challenge dataset from Kaggle](https://www.kaggle.com/competitions/demand-forecasting-kernels-only/data?select=train.csv) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
